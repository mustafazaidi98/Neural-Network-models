{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OpEC24c70IPj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from keras.engine.training import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten,BatchNormalization,Activation,Conv2D,MaxPool2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "model = Sequential()\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('balance-scale.data',header=None);\n",
        "training_data = dataset.sample(frac=0.90, random_state=25)\n",
        "testing_data = dataset.drop(training_data.index)\n",
        "validation_data = training_data[:80];\n",
        "training_data = training_data[80:]\n",
        "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
        "print(f\"No. of testing examples: {testing_data.shape[0]}\")\n",
        "print(f\"No. of validation examples: {validation_data.shape[0]}\")\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "training_labels = encoder.fit_transform(training_data[[0]])\n",
        "training_data.pop(0);\n",
        "testing_labels = encoder.fit_transform(testing_data[[0]])\n",
        "testing_data.pop(0);\n",
        "validation_label = encoder.fit_transform(validation_data[[0]])\n",
        "validation_data.pop(0);\n",
        "np.squeeze(np.array(training_labels,dtype=float))\n",
        "training_data.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMsoIAHF0Tpp",
        "outputId": "a49d8d3c-1cbb-4936-bb25-8d04f3b815ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples: 482\n",
            "No. of testing examples: 63\n",
            "No. of validation examples: 80\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "class ComputationGraphs:\n",
        "  def __init__(self, x, y,node1=2,node2=2):\n",
        "    self.X = x;\n",
        "    self.Y = y\n",
        "    self.ypred = np.zeros((1,self.Y.shape[1]))\n",
        "    self.noOfLayers = 3\n",
        "    self.param = {}\n",
        "    self.ch = {}\n",
        "    self.grad = {}\n",
        "    self.loss = []\n",
        "    self.node1 = node1\n",
        "    self.node2 = node2\n",
        "    self.learningrate = 0.1\n",
        "    self.threshold = 0.5\n",
        "  def nInit(self):\n",
        "    self.param['W1'] = np.ones((self.node1,self.X.shape[1]))*0.03\n",
        "    self.param['W2'] = np.ones((self.node2,self.node1))*0.03\n",
        "    self.param['W3'] = np.ones((self.Y.shape[1],self.node2))*0.03\n",
        "    self.param['b1'] = np.ones((self.node1,1))*0.03\n",
        "    self.param['b2'] = np.ones((self.node2,1))*0.03\n",
        "    self.param['b3'] = np.ones((self.Y.shape[1],1))*0.03\n",
        "  def Sigmoid(self,Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "  def DerivativeSigmoid(self,Z):\n",
        "    h = 1/(1+np.exp(-Z))\n",
        "    dZ = h * (1-h)\n",
        "    return dZ\n",
        "  def Softmax(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        return probabilities;\n",
        "  def DerivateSoftmax(self,x):\n",
        "    x = np.exp(x - np.max(x))\n",
        "    return (np.sum(x)-x)/ np.sum(x)   \n",
        "  def backwardCategoricalCrossentropy(self, dvalues, y_true):\n",
        "        samples = len(dvalues)\n",
        "        labels = len(dvalues[0])\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "        dinputs = -y_true / dvalues\n",
        "        dinputs = dinputs / samples\n",
        "        return dinputs;\n",
        "  def CategoricalCrossentropy(self, y_pred, y_true):\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "  def forwardpass(self):\n",
        "    Z1 = self.param['W1'].dot(self.X.T)\n",
        "    Z1 = Z1 + self.param['b1']\n",
        "    A1 = self.Sigmoid(Z1)\n",
        "    self.ch['Z1'],self.ch['A1'] = Z1, A1\n",
        "\n",
        "    Z2 = self.param['W2'].dot(A1) + self.param['b2']\n",
        "    A2 = self.Sigmoid(Z2)\n",
        "    self.ch['Z2'],self.ch['A2'] = Z2, A2\n",
        "\n",
        "    Z3 = self.param['W3'].dot(A2) + self.param['b3']\n",
        "    A3 = self.Softmax(Z3)\n",
        "    self.ch['Z3'],self.ch['A3'] = Z3, A3\n",
        "    self.ypred = A3\n",
        "    loss = self.CategoricalCrossentropy(self.ypred.T,self.Y)\n",
        "    return self.ypred,loss\n",
        "  def backwardpass(self):\n",
        "    DYpred = self.backwardCategoricalCrossentropy(self.ypred.T , self.Y)\n",
        "    DZ3 = DYpred *self.DerivateSoftmax(self.ch['Z3']).T\n",
        "    DA2 = np.dot(self.param['W3'].T,DZ3.T);\n",
        "    DW3 = 1./self.ch['A2'].shape[1] * np.dot(DZ3.T,self.ch['A2'].T)\n",
        "    DZ2 = DA2 * self.DerivativeSigmoid( self.ch['Z2'])\n",
        "    DA1 = np.dot(self.param['W2'].T,DZ2);\n",
        "    DW2 = 1./self.ch['A1'].shape[1] * np.dot(DZ2,self.ch['A1'].T)\n",
        "\n",
        "    DZ1 = DA1 * self.DerivativeSigmoid( self.ch['Z1'])\n",
        "    DW1 = 1./self.X.shape[1] * np.dot(DZ1,self.X)\n",
        "    self.param[\"W1\"] = self.param[\"W1\"] - self.learningrate * DW1\n",
        "    self.param[\"W2\"] = self.param[\"W2\"] - self.learningrate * DW2\n",
        "    self.param[\"W3\"] = self.param[\"W3\"] - self.learningrate * DW3\n",
        "    return;\n",
        "\n",
        "##Gradient Descent    \n",
        "  def grad_descent(self,X, Y, iter = 8000):\n",
        "    \n",
        "      self.nInit()\n",
        "      Yh = Y;\n",
        "      for i in range(0, iter):\n",
        "          Yh, loss=self.forwardpass()\n",
        "          self.backwardpass()\n",
        "\n",
        "          if i % 500 == 0:\n",
        "            self.loss.append(loss)\n",
        "      print(Yh)      \n",
        "\n"
      ],
      "metadata": {
        "id": "nCMLIEwY1_PP"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cg = ComputationGraphs(training_data,training_labels)\n",
        "cg.grad_descent(training_data,training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eKnmi9JTQnt",
        "outputId": "fce4d374-f309-49e9-8e1a-de4f09d8e94e"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00207469 0.00207469 0.00207469 ... 0.00207469 0.00207469 0.00207469]\n",
            " [0.00207469 0.00207469 0.00207469 ... 0.00207469 0.00207469 0.00207469]\n",
            " [0.00207469 0.00207469 0.00207469 ... 0.00207469 0.00207469 0.00207469]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3xQ3NiSDEz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}